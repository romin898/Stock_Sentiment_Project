{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 1. Searching for gold inside HTML files\n",
    "<p>It used to take days for financial news to spread via radio, newspapers, and word of mouth. Now, in the age of the internet, it takes seconds. Did you know news articles are <em>automatically</em> being generated from figures and earnings call streams? Hedge funds and independent traders are using data science to process this wealth of information in the quest for profit.</p>\n",
    "<p>In this notebook, we will generate investing insight by applying <a href=\"https://en.wikipedia.org/wiki/Sentiment_analysis\">sentiment analysis</a> on financial news headlines from <a href=\"https://finviz.com\">FINVIZ.com</a>. Using this <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">natural language processing</a> technique, we can understand the emotion behind the headlines and predict whether the market <em>feels</em> good or bad about a stock. It would then be possible to make educated guesses on how certain stocks will perform and trade accordingly. (And hopefully, make money!)</p>\n",
    "<p><img src=\"https://assets.datacamp.com/production/project_611/img/fb_headlines.png\" alt=\"Facebook headlines from FINVIZ.com\"></p>\n",
    "<p>Why headlines? And why from FINVIZ?</p>\n",
    "<ol>\n",
    "<li>Headlines, which have similar length, are easier to parse and group than full articles, which vary in length.</li>\n",
    "<li>FINVIZ has a list of trusted websites, and headlines from these sites tend to be more consistent in their jargon than those from independent bloggers. Consistent textual patterns will improve the sentiment analysis.</li>\n",
    "</ol>\n",
    "<p>As <a href=\"https://en.wikipedia.org/wiki/Web_scraping\">web scraping</a> requires data science ethics (sending a lot of traffic to a FINVIZ's servers isn't very nice), the HTML files for Facebook and Tesla at various points in time have been downloaded. Let's import these files into memory.</p>\n",
    "<p><strong>Disclaimer: Investing in the stock market involves risk and can lead to monetary loss. The content in this notebook is not to be taken as financial advice.</strong> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dc": {
     "key": "3"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fb_05ene.html'])\n",
      "dict_keys(['fb_05ene.html', 'fb_22sep.html'])\n",
      "dict_keys(['fb_05ene.html', 'fb_22sep.html', 'tsla_05ene.html'])\n",
      "dict_keys(['fb_05ene.html', 'fb_22sep.html', 'tsla_05ene.html', 'tsla_22sep.html'])\n",
      "dict_keys(['fb_05ene.html', 'fb_22sep.html', 'tsla_05ene.html', 'tsla_22sep.html', 'tsla_26nov.html'])\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "html_tables = {}\n",
    "\n",
    "# For every table in the datasets folder...\n",
    "for table_name in os.listdir('datasets'):\n",
    "    #this is the path to the file. Don't touch!\n",
    "    table_path = f'datasets/{table_name}'\n",
    "    # Open as a python file in read-only mode\n",
    "    table_file =open(table_path,'r')\n",
    "    # Read the contents of the file into 'html'\n",
    "    html = BeautifulSoup(table_file)\n",
    "    # Find 'news-table' in the Soup and load it into 'html_table'\n",
    "    html_table = html.find(id='news-table')\n",
    "    # Add the table to our dictionary\n",
    "    html_tables[table_name] = html_table\n",
    "    print(html_tables.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. What is inside those files anyway?\n",
    "<p>We've grabbed the table that contains the headlines from each stock's HTML file, but before we start parsing those tables further, we need to understand how the data in that table is structured. We have a few options for this:</p>\n",
    "<ul>\n",
    "<li>Open the HTML file with a text editor (preferably one with syntax highlighting, like <a href=\"http://www.sublimetext.com/\">Sublime Text</a>) and explore it there</li>\n",
    "<li>Use your browser's <a href=\"https://addons.mozilla.org/en-US/firefox/addon/web-developer/\">webdev toolkit</a> to explore the HTML</li>\n",
    "<li>Explore the headlines table here in this notebook!</li>\n",
    "</ul>\n",
    "<p>Let's do the third option.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number 1:\n",
      "Billionaire investor questions Elon Musk getting 'a pass' after bombshell tweets\n",
      "Sep-21-18 09:56PM  \n",
      "File number 2:\n",
      "Broadcoms Stock Looks Like a Winner\n",
      "09:30PM  \n",
      "File number 3:\n",
      "SHAREHOLDER ALERT:Â  Pomerantz Law Firm Reminds Shareholders with Losses on their Investment in Tesla, Inc. of Class Action Lawsuit and Upcoming Deadline  TSLA\n",
      "05:30PM  \n",
      "File number 4:\n",
      "Tesla's People Problem and the Inscrutable Musk: 2 Things That Make You Go Hmmm\n",
      "05:30PM  \n"
     ]
    }
   ],
   "source": [
    "# Read one single day of headlines \n",
    "tsla = html_tables['tsla_22sep.html']\n",
    "# Get all the table rows tagged in HTML with <tr> into 'tesla_tr'\n",
    "tsla_tr = tsla.findAll('tr')\n",
    "#print(tsla_tr)\n",
    "\n",
    "\n",
    "# For each row...\n",
    "for i, table_row in enumerate(tsla_tr):\n",
    "    # Read the text of the element 'a' into 'link_text'\n",
    "    link_text =table_row.a.get_text() \n",
    "    # Read the text of the element 'td' into 'data_text'\n",
    "    data_text = table_row.td.get_text()\n",
    "    # Print the count\n",
    "    print(f'File number {i+1}:')\n",
    "    # Print the contents of 'link_text' and 'data_text' \n",
    "    print(link_text)\n",
    "    print(data_text)\n",
    "    # The following exits the loop after four rows to prevent spamming the notebook, do not touch\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Extra, extra! Extract the news headlines\n",
    "<p>As we saw above, the interesting data inside each table row (<code>&lt;tr&gt;</code>) is in the text inside the <code>&lt;td&gt;</code> and <code>&lt;a&gt;</code> tags. Let's now actually parse the data for <strong>all</strong> tables in a comfortable data structure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Hold the parsed news into a list\n",
    "parsed_news = []\n",
    "# Iterate through the news\n",
    "for file_name, news_table in html_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # Read the text from the tr tag into text\n",
    "        text = x.get_text() \n",
    "        # Split the text in the td tag into a list \n",
    "        date_scrape = x.td.text.split()\n",
    "        # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        # If not, load 'date' as the 1st element and 'time' as the second\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "        ticker = file_name.split('_')[0]\n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, x.a.get_text()])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Make NLTK think like a financial journalist\n",
    "<p>Sentiment analysis is very sensitive to context. As an example, saying <em>\"This is so addictive!\"</em> often means something positive if the context is a video game you are enjoying with your friends, but it very often means something negative when we are talking about opioids. Remember that the reason we chose headlines is so we can try to extract sentiment from financial journalists, who like most professionals, have their own lingo. Let's now make NLTK think like a financial journalist by adding some new words and sentiment values to our lexicon.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\romin/nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\.conda\\\\envs\\\\py3env\\\\nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\.conda\\\\envs\\\\py3env\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\.conda\\\\envs\\\\py3env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10664\\1773155418.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Instantiate the sentiment intensity analyzer with the existing lexicon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mvader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Update the lexicon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3env\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mlexicon_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     ):\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVaderConstants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py3env\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\romin/nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\.conda\\\\envs\\\\py3env\\\\nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\.conda\\\\envs\\\\py3env\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\.conda\\\\envs\\\\py3env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\romin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# NLTK VADER for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# New words and values\n",
    "new_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "s=vader.lexicon.update(new_words)\n",
    "# Update the lexicon\n",
    "# ... YOUR CODE FOR TASK 4 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. BREAKING NEWS: NLTK Crushes Sentiment Estimates\n",
    "<p>Now that we have the data and the algorithm loaded, we will get to the core of the matter: programmatically predicting sentiment out of news headlines! Luckily for us, VADER is very high level so, in this case, we will not adjust the model further<sup>*</sup> other than the lexicon additions from before.</p>\n",
    "<p><sup>*</sup>VADER \"out-of-the-box\" with some extra lexicon would likely translate into <strong>heavy losses</strong> with real money. A real sentiment analysis tool with chances of being profitable will require a very extensive and dedicated to finance news lexicon. Furthermore, it might also not be enough using a pre-packaged model like VADER.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Use these column names\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "# Convert the list of lists into a DataFrame\n",
    "scored_news = pd.DataFrame(parsed_news,columns=columns)\n",
    "#print(scored_news.head())\n",
    "# Iterate through the headlines and get the polarity scores\n",
    "scores =[vader.polarity_scores(x) for x in scored_news['headline']]\n",
    "scores_df = pd.DataFrame(scores)\n",
    "#print(scores_df.head())\n",
    "#print(scores_df.head())\n",
    "# Join the DataFrames\n",
    "scored_news = scored_news.merge(scores_df,left_on=scored_news.index,right_on=scores_df.index)\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "del scored_news['key_0']\n",
    "print(scored_news.head())\n",
    "scored_news['date'] = pd.to_datetime(scored_news.date).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Plot all the sentiment in subplots\n",
    "<p>Now that we have the scores, let's start plotting the results. We will start by plotting the time series for the stocks we have.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_c = scored_news.groupby(['date','ticker']).mean()\n",
    "\n",
    "# Unstack the column ticker\n",
    "mean_c = mean_c.unstack('ticker')\n",
    "                       \n",
    "# Get the cross-section of compound in the 'columns' axis\n",
    "mean_c = mean_c.xs(\"compound\",axis='columns')\n",
    "print(mean_c)\n",
    "# Plot a bar chart with pandas\n",
    "mean_c.plot.bar()\n",
    "# ... YOUR CODE FOR TASK 6 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Weekends and duplicates\n",
    "<p>What happened to Tesla on November 22nd? Since we happen to have the headlines inside our <code>DataFrame</code>, a quick peek reveals that there are a few problems with that particular day: </p>\n",
    "<ul>\n",
    "<li>There are only 5 headlines for that day.</li>\n",
    "<li>Two headlines are verbatim the same as another but from another news outlet.</li>\n",
    "</ul>\n",
    "<p>Let's clean up the dataset a bit, but not too much! While some headlines are the same news piece from different sources, the fact that they are written differently could provide different perspectives on the same story. Plus, when one piece of news is more important, it tends to get more headlines from multiple sources. What we want to get rid of is verbatim copied headlines, as these are very likely coming from the same journalist and are just being \"forwarded\" around, so to speak.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Count the number of headlines in scored_news (store as integer)\n",
    "num_news_before = len(scored_news['headline'])\n",
    "# Drop duplicates based on ticker and headline\n",
    "scored_news_clean = scored_news.drop_duplicates(subset=['ticker','headline'])\n",
    "# Count number of headlines after dropping duplicates\n",
    "num_news_after = len(scored_news_clean['headline'])\n",
    "# Print before and after numbers to get an idea of how we did \n",
    "f\"Before we had {num_news_before} headlines, now we have {num_news_after}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. Sentiment on one single trading day and stock\n",
    "<p>Just to understand the possibilities of this dataset and get a better feel of the data, let's focus on one trading day and one single stock. We will make an informative plot where we will see the smallest grain possible: headline and subscores.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the index to ticker and date\n",
    "single_day = scored_news_clean.set_index(['ticker', 'date'])\n",
    "\n",
    "# Cross-section the fb row\n",
    "single_day = single_day.xs('fb',axis='rows')\n",
    "# Select the 3rd of January of 2019\n",
    "single_day = single_day['2019-01-03']\n",
    "# Convert the datetime string to just the time\n",
    "single_day['time'] = pd.to_datetime(single_day.time).dt.time\n",
    "# Set the index to time and \n",
    "single_day = single_day.set_index('time')\n",
    "# Sort it\n",
    "single_day = single_day.sort_index()\n",
    "print(single_day.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 9. Visualize the single day\n",
    "<p>We will make a plot to visualize the positive, negative and neutral scores for a single day of trading and a single stock. This is just one of the many ways to visualize this dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "59"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "TITLE = \"Negative, neutral, and positive sentiment for FB on 2019-01-03\"\n",
    "COLORS = [\"red\",\"orange\", \"green\"]\n",
    "# Drop the columns that aren't useful for the plot\n",
    "plot_day = single_day.drop(['headline','compound'],axis=1)\n",
    "# Change the column names to 'negative', 'positive', and 'neutral'\n",
    "print(plot_day)\n",
    "plot_day.columns = ['negative','neutral','positive']\n",
    "print(plot_day)\n",
    "# Plot a stacked bar chart\n",
    "plot_day.plot.bar(stacked=True,title=TITLE,color=COLORS)\n",
    "# ... YOUR CODE FOR TASK 9 :-) ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
